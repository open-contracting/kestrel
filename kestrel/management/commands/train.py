import random
import re
import shutil
import tempfile
from pathlib import Path

import nltk
import numpy as np
from django_rich.management import RichCommand
from sklearn.feature_extraction.text import TfidfVectorizer, strip_accents_unicode
from sklearn.metrics import average_precision_score, classification_report
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from skops.io import dump

try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords")

from nltk.corpus import stopwords

TOP_N = 20

URL_OR_EMAIL = re.compile(r"http\S+|www\S+|https\S+|\S+@\S+")

# If changing metric, update occurrences of `average_precision_score`.
#
# AP is close to "area under the precision-recall curve", which is appropriate for imbalanced data where only the
# minority class is of interest ("procurement-related"). "f1" is another option.
# https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-and-f-measures
CV_SCORING = "average_precision"

# Add parameters during development.
PARAM_GRID = {}


def preprocessor(text):
    """Remove noisy text."""
    # If the `preprocessor` is set in `TfidfVectorizer`, `lowercase` and `strip_accents` are ignored.
    # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.build_preprocessor
    text = strip_accents_unicode(text.lower())

    # TODO(james): Remove signatures, salutations, phone numbers, addresses.
    # https://github.com/open-contracting/kestrel/issues/2
    # Signatures contain the requester's name, which could be a confounder.
    return URL_OR_EMAIL.sub("", text)


class Command(RichCommand):
    help = "Train and cross-validate a classifier to label text about procurement"

    def add_arguments(self, parser):
        # GridSearchCV uses joblib, which pickles top-level imports, and unpickling raises AppRegistryNotReady.
        from kestrel.models import SOURCES  # noqa: PLC0415

        parser.add_argument("source", choices=SOURCES, help="Source for which to train a classifier")
        parser.add_argument(
            "--language", choices=stopwords.fileids(), default="english", help="The language of the stopwords to use"
        )

    def handle(self, *args, **options):
        from kestrel.models import Record  # noqa: PLC0415

        source = options["source"]
        language = options["language"]
        verbosity = options["verbosity"]

        X = []
        y = []
        for record in Record.objects.filter(source=source).exclude(manual_label=None):
            response = record.response
            match source:
                case "muckrock_foia":
                    # The first email's subject and body are the most relevant. Later emails are noisy and numerous,
                    # even after excluding `autogenerated` emails. If adding others, it might be best to have separate
                    # features for emails from/to the user.
                    #
                    # Record title and tags are assigned by users and not sent to agencies, and therefore might be
                    # confounders. Other fields are either irrelevant (like dates), noisy (like filenames) or often
                    # missing (like file descriptions).
                    if communications := response["communications"]:
                        # TODO(james): A FeatureUnion could be used to keep the fields distinct.
                        # https://github.com/open-contracting/kestrel/issues/3
                        X.append(f"{communications[0]['subject']} {communications[0]['communication']}")
                        y.append(bool(record.manual_label))
                case _:
                    raise NotImplementedError

        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            # Set `random_state` so hyperparameter combinations are evaluated on the same split in the grid search.
            # https://scikit-learn.org/stable/common_pitfalls.html#controlling-randomness
            # https://scikit-learn.org/stable/glossary.html#term-random_state
            random_state=random.randrange(2**32) if PARAM_GRID else None,  # noqa: S311
            # NOTE: If the labeled data is imbalanced, use `train_test_split(stratify=y)`.
            # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
        )

        # Allow the user to verify that the classes are balanced.
        self.stdout.write(f"Class distribution:\n{np.asarray(np.unique(y_train, return_counts=True)).T}")

        cache_dir = tempfile.mkdtemp()
        try:
            # Use a pipeline to avoid inconsistent preprocessing and data leakage.
            # https://scikit-learn.org/stable/common_pitfalls.html
            pipeline = Pipeline(
                [
                    (
                        "tfidf",
                        # Use the default tokenizer. (NLTK's word_tokenize() splits contractions like "can't" into
                        # "ca" and "n't", but its stopwords include "can" not "ca".)
                        TfidfVectorizer(
                            preprocessor=preprocessor,
                            # The default "english" stopword list has known issues.
                            # https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words
                            stop_words=stopwords.words(language),
                            # Too many features relative to the sample size can lead to overfitting (e.g. >4x).
                            # If increased, consider increasing `min_df` from its default of 1.
                            max_features=500,
                            # The validation curve has higher variance above 0.7 and lower score above 0.8.
                            # A lower `max_df` could be due to the repetitive wording of FOIA requests.
                            max_df=0.8,
                            # Bigrams help disambiguate, e.g. "collective bargaining agreements".
                            ngram_range=(1, 2),
                            # Recommended with MultinomialNB in case of long texts with high-frequency words.
                            sublinear_tf=True,
                        ),
                    ),
                    # NOTE: For imbalanced data, consider `class_prior`, using known priors or compute_class_weight().
                    ("est", MultinomialNB(alpha=0.1)),
                ],
                # https://scikit-learn.org/stable/modules/compose.html#caching-transformers-avoid-repeated-computation
                memory=cache_dir,
                # `verbose=True` logs timing information.
            )

            if PARAM_GRID:
                # If performance degrades, see: https://github.com/open-contracting/kestrel/issues/4
                pipeline = GridSearchCV(
                    pipeline, param_grid=PARAM_GRID, scoring=CV_SCORING, n_jobs=-1, verbose=verbosity
                )

            pipeline.fit(X_train, y_train)
        finally:
            shutil.rmtree(cache_dir)

        # TIP: Prompt an agent to "Add visualizations and curves to compare the performance of different parameters
        # in @kestrel/management/commands/train.py". This is simpler than writing one-size-fits-all reports.

        if PARAM_GRID:
            self.stdout.write("Best params:")
            for key, value in pipeline.best_params_.items():
                self.stdout.write(f"  {key}: {value}")

            mean_cv_score = pipeline.best_score_
            std_cv_score = pipeline.cv_results_["std_test_score"][pipeline.best_index_]
            test_score = pipeline.score(X_test, y_test)
            estimator = pipeline.best_estimator_
        else:
            # If performance degrades, consider `n_jobs=-1` and `verbose=verbosity`.
            cv_scores = cross_val_score(pipeline, X_train, y_train, scoring=CV_SCORING)

            mean_cv_score = cv_scores.mean()
            std_cv_score = cv_scores.std()
            # Although both GridSearchCV and Pipeline have a `score` method, GridSearchCV uses the `scoring` metric,
            # but Pipeline calls MultinomialNB's `score` method, which uses the "accuracy" metric.
            test_score = average_precision_score(y_test, pipeline.predict_proba(X_test)[:, 1])
            estimator = pipeline

        self.stdout.write(f"CV score: {mean_cv_score:.3f} Â± {std_cv_score:.3f}")
        self.stdout.write(f"Test score: {test_score:.3f}")

        self.stdout.write("\nClassification report:")
        self.stdout.write(classification_report(y_test, estimator.predict(X_test)))

        feature_names = estimator.named_steps["tfidf"].get_feature_names_out()
        log_probabilities = estimator.named_steps["est"].feature_log_prob_
        scores = log_probabilities[1] - log_probabilities[0]
        indexes = scores.argsort()
        self.stdout.write("\nTop features:")
        for i, idx in enumerate(reversed(indexes[-TOP_N:]), 1):
            self.stdout.write(f"{i:3d}. {feature_names[idx]:25s} ({scores[idx]:.3f})")
        self.stdout.write("...")
        for i, idx in enumerate(reversed(indexes[:TOP_N]), len(feature_names) - TOP_N + 1):
            self.stdout.write(f"{i:3d}. {feature_names[idx]:25s} ({scores[idx]:.3f})")

        if not PARAM_GRID:
            # WARNING: Do not load models across different versions of sklearn.
            # https://skops.readthedocs.io/en/stable/persistence.html#backwards-compatibility
            #
            # We can't use ONNX: "Custom preprocessor cannot be converted into ONNX".
            # https://scikit-learn.org/stable/model_persistence.html
            path = Path("models") / f"{source}.skops"
            dump(pipeline, path)
            self.console.print(f"Saved model to {path}", style="green")
